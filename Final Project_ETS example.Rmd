---
title: "Final Project ETS and TBATS"
output: html_document
date: "2023-11-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Clear working memory
rm(list=ls())

library(tidyverse)
library(fpp)
library(tseries)
library(ggplot2)
library(forecast)
library(plotly)
library(readxl)
library(xts)
library(TSA)
```

```{r}
# set directory
#C:/Users/nugra/OneDrive - The University of Chicago/UChicago/Time Series HW/Final Project

setwd('C:/Users/Shofi/Desktop/Fall 2023/ADSP 31006 - Time Series Analysis and Forecasting/Final Project/forecast-tourist-arrival')
getwd()
```

```{r}
# LOAD THE DATA
df_arrival <-read_xlsx('arrival.xlsx')

head(df_arrival)

# CREATING TIME SERIES DATA
arrival <- ts(df_arrival$arrival, start = c(2000,1), frequency = 12)
head(arrival)

# PLOTTING
autoplot(arrival, main = "Monthly Tourist Arrival in Indonesia, January 2000 - December 2019", 
         ylab = "Tourist Arrival", xlab = "Year")

seasonplot(arrival, main = "Seasonal Plot of Tourist Arrival Indonesia, 
           January 2000 - December 2019")

```

```{r}
# CHECKING THE NORMALITY
hist(arrival, main = "Histogram of Foreign Tourists Arrival in Indonesia,
     January 2000 - December 2019")

shapiro.test(arrival)
```

```{r}
# CUT AND SPLIT THE DATA
training_arrival <- window(arrival,  start = c(2000,1), end = c(2015,12))

test_arrival <- window(arrival, start= c(2016,1), end = c(2019,12))

autoplot(training_arrival)
autoplot(test_arrival)

seasonplot(training_arrival)

# CHECKING THE NORMALITY
hist(training_arrival, main = "Histogram of Foreign Tourists Arrival in Indonesia,
     January 2000 - December 2015")

shapiro.test(training_arrival)

# CHECKING STATIONARITY
tsdisplay(training_arrival, main = "ACF and PACF for Foreign Tourists Arrival in Indonesia,
     January 2000 - December 2015")

acf(training_arrival)
pacf(training_arrival)
kpss.test(training_arrival)
adf.test(training_arrival)

```

```{r}
# BOX COX TRANSFORMATION
auto_lambda <- BoxCox.lambda(training_arrival)
auto_lambda

training_arrival_bcauto <- BoxCox(training_arrival,auto_lambda)
plot(training_arrival_bcauto)
hist(training_arrival_bcauto)
shapiro.test(training_arrival_bcauto)
kpss.test(training_arrival_bcauto, null=c("Level"))

acf(training_arrival_bcauto)
```

```{r}

#BENCHMARK ETS MODEL - LAMBDA 0

#model
model_ets_1 <- ets(training_arrival, model="ZZZ", lambda = 0)
fcast_ets_1 <- forecast(model_ets_1, h=48)
summary(model_ets_1)
aicc_ets_1 <- model_ets_1$aicc
bic_ets_1 <- model_ets_1$bic
checkresiduals(model_ets_1)

#plot
plot(model_ets_1$fitted, type = "l", col = "blue", lwd = 2, xlab = "Time", ylab = "Values", main = "Fitted Plot")
lines(training_arrival, type = "l", col = "red", lwd = 2)
grid()
legend("topleft", legend = c("Fitted", "Actual"), col = c("blue", "red"), lty = 1, lwd = 2)

#accuracy forecast
accuracy_ets_1 <- accuracy(fcast_ets_1, test_arrival)
accuracy_ets_1
rmse_ets_1 <- accuracy_ets_1[4]
mape_ets_1 <- accuracy_ets_1[10]
mse_ets_1 <- mean(abs((test_arrival - fcast_ets_1$mean))^2)

#plot
plot(fcast_ets_1, type = "l", col = "blue", lwd = 2, xlab = "Time", ylab = "Values", main = "Forecast Plot")
lines(test_arrival, type = "l", col = "red", lwd = 2)
grid()
legend("topleft", legend = c("Forecast", "Test Arrival"), col = c("blue", "red"), lty = 1, lwd = 2)
```

```{r}

#BENCHMARK ETS MODEL - LAMBDA auto

#model
model_ets_2 <- ets(training_arrival, model="ZZZ", lambda = "auto")
fcast_ets_2 <- forecast(model_ets_2, h=48)
summary(model_ets_2)
aicc_ets_2 <- model_ets_2$aicc
bic_ets_2 <- model_ets_2$bic
checkresiduals(model_ets_2)

#plot
plot(model_ets_2$fitted, type = "l", col = "blue", lwd = 2, xlab = "Time", ylab = "Values", main = "Fitted Plot")
lines(training_arrival, type = "l", col = "red", lwd = 2)
grid()
legend("topleft", legend = c("Fitted", "Actual"), col = c("blue", "red"), lty = 1, lwd = 2)

#accuracy forecast
accuracy_ets_2 <- accuracy(fcast_ets_2, test_arrival)
accuracy_ets_2
rmse_ets_2 <- accuracy_ets_2[4]
mape_ets_2 <- accuracy_ets_2[10]
mse_ets_2 <- mean(abs((test_arrival - fcast_ets_2$mean))^2)

#plot
plot(fcast_ets_2, type = "l", col = "blue", lwd = 2, xlab = "Time", ylab = "Values", main = "Forecast Plot")
lines(test_arrival, type = "l", col = "red", lwd = 2)
grid()
legend("topleft", legend = c("Forecast", "Test Arrival"), col = c("blue", "red"), lty = 1, lwd = 2)
```

```{r}

#BENCHMARK ARIMA - LAMBDA 0

#model
model_arima_1 <- auto.arima(training_arrival, seasonal=TRUE, lambda = 0)
fcast_arima_1 <- forecast(model_arima_1, h=48)
summary(model_arima_1)
aicc_arima_1 <- model_arima_1$aicc
bic_arima_1 <- model_arima_1$bic
checkresiduals(model_arima_1)

```

```{r}
#BENCHMARK ARIMA - LAMBDA AUTO

#model
model_arima_1 <- auto.arima(training_arrival, seasonal=TRUE, lambda = "auto")
fcast_arima_1 <- forecast(model_arima_1, h=48)
summary(model_arima_1)
aicc_arima_1 <- model_arima_1$aicc
bic_arima_1 <- model_arima_1$bic
checkresiduals(model_arima_1)

```

**In this section you will apply the time-series cross validation method to train and test various models. Use the following values when training and testing the models:**

**• Set the minimum number of samples required to train the model to 160 (i.e., this is the minimum number of samples in the sliding window and the initial number of samples in the expanding window method.)**

**• Set the number the forecast horizon, ℎ, to 1 year (i.e., 12 months.)**

**• Recall that the period, 𝑝, is equal to 12 months**

**• Use a single observation incrementation in each iteration (i.e., shift the training set forward by 1 observation.)**

**• Note: You are expected to have 80 iterations of cross validation**

```{r}
k <- 192 # minimum data length for fitting a model
n <- length(arrival) # Number of data points

p <- 12 ### Period
H <- 48 # Forecast Horiz
```

```{r}
defaultW <- getOption("warn") 
options(warn = -1)

st <- tsp(arrival)[1]+(k-2)/p #  gives the start time in time units,

mae_1 <- matrix(NA,n-k,H)
mae_2 <- matrix(NA,n-k,H)
mae_3 <- matrix(NA,n-k,H)
mae_4 <- matrix(NA,n-k,H)
mae_5 <- matrix(NA,n-k,H)
mae_6 <- matrix(NA,n-k,H)
mae_7 <- matrix(NA,n-k,H)
mae_8 <- matrix(NA,n-k,H)

rmse_1 <- matrix(NA,n-k,H)
rmse_2 <- matrix(NA,n-k,H)
rmse_3 <- matrix(NA,n-k,H)
rmse_4 <- matrix(NA,n-k,H)
rmse_5 <- matrix(NA,n-k,H)
rmse_6 <- matrix(NA,n-k,H)
rmse_7 <- matrix(NA,n-k,H)
rmse_8 <- matrix(NA,n-k,H)

AICc_1 <- matrix(NA,n-k,H)
AICc_2 <- matrix(NA,n-k,H)
AICc_3 <- matrix(NA,n-k,H)
AICc_4 <- matrix(NA,n-k,H)
AICc_5 <- matrix(NA,n-k,H)
AICc_6 <- matrix(NA,n-k,H)
AICc_7 <- matrix(NA,n-k,H)
AICc_8 <- matrix(NA,n-k,H)

for(i in 1:(n-k))
{
  
  ### One Month rolling forecasting
  # Expanding Window 
  train_1 <- window(arrival, end=st + i/p)  ## Window Length: k+i
  
  # Sliding Window - keep the training window of fixed length. 
  # The training set always consists of k observations.
  train_2 <- window(arrival, start=st+(i-k+1)/p, end=st+i/p) ## Window Length: k
  
  #Test
  test <- window(arrival, start=st + (i+1)/p, end=st + (i+H)/p) ## Window Length: H

  
  ### ETS: LAMBDA 0
  #### EXPANDING TRAINING WINDOW
  fit_1 <- ets(train_1, lambda = 0) #MODEL
  fcast_1 <- forecast(fit_1, h=H) #FORECAST
  AICc_1[i,1:length(test)] <- fit_1$aicc  #AICc
  mae_1[i,1:length(test)] <- abs(fcast_1[['mean']]-test) #MAE
  rmse_1[i,1:length(test)] <- sqrt(mean((fcast_1[['mean']]-test)^2)) #RMSE
  
  ### ETS: LAMBDA 0
  #### SLIDING TRAINING WINDOW
  fit_2 <- ets(train_2, lambda = 0) #MODEL
  fcast_2 <- forecast(fit_2, h=H) #FORECAST
  AICc_2[i,1:length(test)] <- fit_2$aicc #AICc 
  mae_2[i,1:length(test)] <- abs(fcast_2[['mean']]-test) #MAE
  rmse_2[i,1:length(test)] <- sqrt(mean((fcast_2[['mean']]-test)^2)) #RMSE

  ### ETS: LAMBDA AUTO
  #### EXPANDING TRAINING WINDOW
  fit_3 <- ets(train_1, lambda = "auto") #MODEL
  fcast_3 <- forecast(fit_3, h=H)
  AICc_3[i,1:length(test)] <- fit_3$aicc
  mae_3[i,1:length(test)] <- abs(fcast_3[['mean']]-test)
  rmse_3[i,1:length(test)] <- sqrt(mean((fcast_3[['mean']]-test)^2)) #RMSE
  
  ### ETS: LAMBDA AUTO
  #### SLIDING TRAINING WINDOW
  fit_4 <- ets(train_2, lambda = "auto") #MODEL
  fcast_4 <- forecast(fit_4, h=H) #FORECAST
  AICc_4[i,1:length(test)] <- fit_4$aicc #AICc
  mae_4[i,1:length(test)] <- abs(fcast_4[['mean']]-test) #MAE
  rmse_4[i,1:length(test)] <- sqrt(mean((fcast_4[['mean']]-test)^2)) #RMSE
  
  ### ARIMA: LAMBDA 0
  #### EXPANDING TRAINING WINDOW
  fit_5 <- auto.arima(train_1, seasonal=TRUE, lambda = 0) #MODEL
  fcast_5 <- forecast(fit_5, h=H) #FORECAST
  AICc_5[i,1:length(test)] <- fit_5$aicc  #AICc
  mae_5[i,1:length(test)] <- abs(fcast_5[['mean']]-test) #MAE
  rmse_5[i,1:length(test)] <- sqrt(mean((fcast_5[['mean']]-test)^2)) #RMSE
  
  ### ARIMA: LAMBDA 0
  #### SLIDING TRAINING WINDOW
  fit_6 <- auto.arima(train_2, seasonal=TRUE, lambda = 0)  #MODEL
  fcast_6 <- forecast(fit_6, h=H) #FORECAST
  AICc_6[i,1:length(test)] <- fit_6$aicc #AICc 
  mae_6[i,1:length(test)] <- abs(fcast_6[['mean']]-test) #MAE
  rmse_2[i,1:length(test)] <- sqrt(mean((fcast_6[['mean']]-test)^2)) #RMSE

  ### ARIMA: LAMBDA AUTO
  #### EXPANDING TRAINING WINDOW
  fit_7 <- auto.arima(train_1, seasonal=TRUE, lambda = "auto") #MODEL
  fcast_7 <- forecast(fit_7, h=H)
  AICc_7[i,1:length(test)] <- fit_7$aicc
  mae_7[i,1:length(test)] <- abs(fcast_7[['mean']]-test)
  rmse_7[i,1:length(test)] <- sqrt(mean((fcast_7[['mean']]-test)^2)) #RMSE
  
  ### ARIMA: LAMBDA AUTO
  #### SLIDING TRAINING WINDOW
  fit_8 <- auto.arima(train_2, seasonal=TRUE, lambda = "auto") #MODEL
  fcast_8 <- forecast(fit_8, h=H) #FORECAST
  AICc_8[i,1:length(test)] <- fit_8$aicc #AICc
  mae_8[i,1:length(test)] <- abs(fcast_8[['mean']]-test) #MAE
  rmse_8[i,1:length(test)] <- sqrt(mean((fcast_8[['mean']]-test)^2)) #RMSE
  
    
}
```

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAaCAYAAADFTB7LAAAAa0lEQVR42u3OywnAIBBAwcXSUoCW5D11xDoNCBGNv0MOecJOBSOi1OZMsJ4dvFxEJ1OQnMxBarIKEpNNkJbsBknJYZCSnAYJyVVQziNig7/nZkFEbhTE5HpBVO4dxOXKIDL3BLG5BJ1T6rsbMfep2CaMN00AAAAASUVORK5CYII= "Run Current Chunk")

```{r}
plot(1:48, colMeans(mae_1, na.rm=TRUE), type="l", 
     main = "MAE vs forecast horizon plots", xlab='Forecast Horizon Plots', ylab='MAE')
lines(1:48, colMeans(mae_2,na.rm=TRUE), type="l",col=2)
lines(1:48, colMeans(mae_3,na.rm=TRUE), type="l",col=3)
lines(1:48, colMeans(mae_4,na.rm=TRUE), type="l",col=4)
lines(1:48, colMeans(mae_5,na.rm=TRUE), type="l",col=5)
lines(1:48, colMeans(mae_6,na.rm=TRUE), type="l",col=6)
lines(1:48, colMeans(mae_7,na.rm=TRUE), type="l",col=7)
lines(1:48, colMeans(mae_8,na.rm=TRUE), type="l",col=8)
legend("topleft",legend=c("ETS: 0 - EXPANDING","ETS: 0 - SLIDING", "ETS: AUTO - EXPANDING","ETS: AUTO - SLIDING","SARIMA: 0 - EXPANDING","SARIMA: 0 - SLIDING", "SARIMA: AUTO - EXPANDING","SARIMA: AUTO - SLIDING"),col=1:8,lty=1, cex=1)
```

Out of all models, Seasonal ARIMA with sliding window has the lowest Mean Absolute Error, indicating it is the Seasonal ARIMA with sliding window is the most accurate model to make predictions.

For cross validation method, sliding window in Seasonal ARIMA and ETS has lower MAE than expanding window. Thus, we can infer that the model will be more accurate if it's trained on most recent observations (sliding window).

For all models, Mean Absolute Forecast Error increases over forecast horizon, so these models are better in forecasting short-time horizon.

```{r}
plot(1:48, colMeans(rmse_1,na.rm=TRUE), type="l", 
     main = "RMSE vs forecast horizon plots", xlab='Forecast Horizon Plots', ylab='RMSE')
lines(1:48, colMeans(rmse_2,na.rm=TRUE), type="l",col=2)
lines(1:48, colMeans(rmse_3,na.rm=TRUE), type="l",col=3)
lines(1:48, colMeans(rmse_4,na.rm=TRUE), type="l",col=4)
lines(1:48, colMeans(rmse_5,na.rm=TRUE), type="l",col=5)
lines(1:48, colMeans(rmse_6,na.rm=TRUE), type="l",col=6)
lines(1:48, colMeans(rmse_7,na.rm=TRUE), type="l",col=7)
lines(1:48, colMeans(rmse_8,na.rm=TRUE), type="l",col=8)
legend("topleft",legend=c("ETS: 0 - EXPANDING","ETS: 0 - SLIDING", "ETS: AUTO - EXPANDING","ETS: AUTO - SLIDING","SARIMA: 0 - EXPANDING","SARIMA: 0 - SLIDING", "SARIMA: AUTO - EXPANDING","SARIMA: AUTO - SLIDING"),col=1:8,lty=1, cex=0.8)
```

Out of all models, Seasonal ARIMA with sliding window has the lowest Root Mean Square Error, indicating it is the Seasonal ARIMA with sliding win

```{r}
plot(1:48, rowMeans(AICc_1,na.rm=TRUE), type="l", 
     main = "AICc vs number of iterations", xlab='Number of iterations', ylab='AICc')
lines(1:48, rowMeans(AICc_2,na.rm=TRUE), type="l",col=2)
lines(1:48, rowMeans(AICc_5,na.rm=TRUE), type="l",col=3)
lines(1:48, rowMeans(AICc_6,na.rm=TRUE), type="l",col=4)
legend("topleft",legend=c("ETS: 0 - EXPANDING","ETS:0 - SLIDING","SARIMA: 0 - EXPANDING","SARIMA: 0 - SLIDING"),col=1:4,lty=1, cex=0.8)
```

```{r}
plot(1:48, rowMeans(AICc_3,na.rm=TRUE), type="l", 
     main = "AICc vs number of iterations")
lines(1:48, rowMeans(AICc_4,na.rm=TRUE), type="l",col=2)
lines(1:48, rowMeans(AICc_7,na.rm=TRUE), type="l",col=3)
lines(1:48, rowMeans(AICc_8,na.rm=TRUE), type="l",col=4)
legend("topleft",legend=c("ETS: AUTO - EXPANDING","ETS:AUTO - SLIDING","SARIMA:AUTO - EXPANDING","SARIMA:AUTO - SLIDING"),col=1:4,lty=1, cex=0.8)
```

ETS sliding window is a better model than ETS expanding window because it has lower AICc. ETS sliding window is relatively more stable while ETS expanding window has higher AICc as the number of iteration increase.

Out of all models, Seasonal ARIMA with sliding model cross validation method is the best model because it has the lowest AICc.

```{r}
checkresiduals(fit_1)
checkresiduals(fit_2)
checkresiduals(fit_3)
checkresiduals(fit_4)
checkresiduals(fit_5)
checkresiduals(fit_6)
checkresiduals(fit_7)
checkresiduals(fit_8)
```
